\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[left=1.25in,top=1.25in,right=1.25in,bottom=1.25in,head=1.25in]{geometry}
\usepackage{amsmath,amssymb,amsthm}

\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{tcolorbox}
\usepackage{algpseudocode}

\newcommand{\incfig}[1]{%
    \def\svgwidth{\columnwidth}
    \import{./figures/}{#1.pdf_tex}
}

\newcommand{\floor}[1]{\lfloor #1 \rfloor}

\newtheorem{exo}{Exercise}
\def\P{\mathbb{P}}
\def\E{\mathbb{E}}

\title{Randomised Algorithms \\
Winter term 2022/2023, Exercise Sheet No. 9}

\author{
    \textbf{Authors:} \\
    Ben Ayad, Mohamed Ayoub \\
    Kamzon, Noureddine
}

\begin{document}
\maketitle

\begin{exo}{\ \\}
\noindent
\textbf{(a)} As $\E[X] = \frac{n}{2}$, we have:

\[\P[X \geq \frac{3}{4} n] 
    =  \P[X \geq (1 + \frac{1}{2}) \frac{n}{2}] 
    = \P[X \geq (1+\frac{1}{2}) \E[X])]\]
And similarily:

\[\P[X \leq \frac{1}{4} n] 
    = \P[X \leq (1-\frac{1}{2}) \E[X])]\]

Using the Chernoff bounds, (Ineq 2 then 3, from Slides 4), we conclude that (for $\delta = 1/2 <  1$):

\[
\P[X \leq \frac{1}{4} n] \leq e^{-\frac{n}{24}}
\quad \text{ and }  \quad
\P[X \geq \frac{3}{4} n] \leq  e^{-\frac{n}{24}}
\]

\noindent
\textbf{(b)} Using the the simplified Chernoff bounds:

\begin{align*}
\P[X \geq n/2 +2\sqrt{n}]   
&= \P[X \geq \frac{n}{2}(1 + \frac{4}{\sqrt{n}})] \\
&\leq e^{ - \frac{n}{2} \left( \frac{4}{\sqrt{n}} \right)^2 \frac{1}{3}} 
\quad \text{(Requires $\frac{4}{\sqrt{n}} < 1 \implies n > 16$)}
\\
&= e^{-8/3}
\end{align*}

Using the additive Chernoff (No extra requirements needed):
\begin{align*}
\P[X \geq n/2 +2\sqrt{n}]   
&\leq e^{-2 \left(2 \sqrt{n} \right)^2 \frac{1}{n}}\\
&= e^{-8}
\end{align*}

\noindent
\textbf{(c)} We let $n_1 = \floor{n/2 - \sqrt{n}/4}$ and $n_2 = \floor{n/2 + \sqrt{n}/4}$, we have:

\begin{align*}
    \P[X \in [n/2 - \sqrt{n}/4, n/2 + \sqrt{n}/4]]
    &\leq \P[X \in [n_1, n_2]] \\
    &= \sum^{n_2}_{k=n_1} \P[X = k] \\
    &= \sum^{n_2}_{k = n_1} \binom{n}{k} \frac{1}{2^n} \\
    &\leq \frac{1}{2^n} \sum^{n_2}_{k = n_1} 2^n \frac{\sqrt{2}}{\sqrt{n}}  
    \quad \text{(Using the hint)}
    \\
    &= \frac{\sqrt{2}}{\sqrt{n}} \sum^{n_2}_{k=n_1} 1  
    = \frac{\sqrt{2}}{\sqrt{n}} (n_2 - n_2 +1) \\
    &\leq \frac{\sqrt{2}}{\sqrt{n}}  
    \left(n/2 + \sqrt{n}/4 - (n/2 - \sqrt{n}/4 - 1)\right) 
    \text{ (using: $\floor{x}-\floor{y} \leq x-(y-1)$)}
    \\
    &= \frac{\sqrt{2}}{\sqrt{n}} (1 + \sqrt{n}/2)\\
    &= \frac{\sqrt{2}}{\sqrt{n}}  + \frac{1}{\sqrt{2}} 
\end{align*}

For $n_0  = 24$, we have for 
$n \geq n_0: 
\frac{\sqrt{2}}{\sqrt{n}} + \frac{1}{\sqrt{2}} 
\leq \frac{\sqrt{2}}{\sqrt{24}} + \frac{1}{\sqrt{2}} 
\approx 0.995 $

\end{exo}

\begin{exo}{\ \\}
    Let $X_i$ be the indicator random variable telling whether the i-th queried person is voting for Alice or not, we have $A = \sum^{k}_{i=1}  X_i$. As $X_i$'s are drawn randomly with replacement, 
    they are considered i.i.d realisations of the true distribution, i.e., $X_i \sim Bern(p) $.

We also have: $\E[A] = kp$, hence, $\hat{p} = A/k$ is an unbiased estimate of $p$.

We have:
\begin{align*}
    \P[|\hat{p}-p|>\epsilon]
    &= \P[|k\hat{p}-kp|>k\epsilon] \\
    &= \P\left[|A - \E[A]| > \E[A] \frac{k \epsilon}{\E[A]} \right] \\
    &= \P\left[|A - \E[A]| > \E[A] \frac{\epsilon}{p} \right] \\
\end{align*}

We have proved in class the following for $0< \delta<1$:
\[
\P[X \geq (1+\delta)\E[X]] \leq \left( \frac{e^\delta}{(1+\delta)^{(1+\delta)}}\right)
^{\E[X]}
\quad \text{And} \quad
\P[X \leq (1-\delta)\E[X]] \leq \left( \frac{e^{-\delta}}{(1-\delta)^{(1-\delta)}}\right)
^{\E[X]}
\]

Using the inequality $\frac{2\delta}{2+\delta} \leq \log{(1+\delta)}$, we can easily upper bound the past inequalities that we haave proved in class to the following versions of Chernoff:

\[
\P[X \geq (1+\delta)\E[X]] 
\leq 
\exp(-\frac{\delta^2 \E[X]}{2+\delta})
\quad \text{And} \quad
\P[X \leq (1-\delta)\E[X]] 
\leq 
\exp(-\frac{\delta^2 \E[X]}{2})
\]

And hence:

\[
\P\left[|X-\E[X]| \geq \mu \E[X]\right] 
\leq 2 \exp{(-\frac{\delta^2 \E[X]}{2+\delta})}
\]

Which yields:
\begin{align*}
    \P[|\hat{p}-p|>\epsilon]
    &= \P\left[|A - \E[A]| > \E[A] \frac{\epsilon}{p} \right] \\
    &\leq 2 \exp{\left(- \frac{(\epsilon/p)^2 \E[X]}{2 + (\epsilon/p)} \right)} \\
    &\leq 2 \exp{\left(- \frac{\epsilon^2 k}{2p + \epsilon} \right)}
\end{align*}


%    &\leq 2 \exp{\left(-2\E[A] \left(\frac{\epsilon}{k}\right)^2 \frac{1}{3}\right)}
%    \quad \text{(using Chernoff bounds)} \\
%    &= 2 \exp{\left(-2 k p   \left(\frac{\epsilon}{k}\right)^2 \frac{1}{3}\right)} \\
%    &= 2 \exp{\left(-\frac{2 \epsilon^2 p}{3k}\right)}
%\end{align*}
\end{exo}

\begin{exo}{\ \\}
Let the RV $X$ represent the sum of the $n$ rolls, we have established previously that: $\E[X] = \frac{7n}{2}$


Using Hoeffding's inequality leads to:

\begin{align*}
    \P[X \geq 4n] 
    &= \P[X \geq E[X]+ \frac{n}{2} ] \\
    &\leq \exp{\left(-\frac{2 (n/2)^2}{\sum^{n}_{i=1} (6-1)^2}\right)} \\
    &= e^{-n/50} 
\end{align*}

This tail that we obtained using Hoeffding's inequality decreases way faster than what we have obtained with Marokov and Chebychev's inequalities (respectively $\frac{7}{8}$ and $\frac{35}{3n}$).

\end{exo}

\begin{exo}{\ \\}

    Let's first establish a few results on $d(u,v)$ for two random vertices of the hypercube. $d(u,v)$ represents the number of bits $v_i$ in $v$ that are diffrent than $u_i$, hence, $d(u,v) \in \{0, \dots, n\}$, and $\P[d(u,v) = k] = \frac{\binom{n}{k}}{2^n}$.(Why: How many ways can we pick k positions so we can flip their bits in $v$ divided by the number of all configurations.)

    By noticing that $\sum^{n}_{i=1} k \binom{n}{k} = n 2^{n-1}$, we can conclude that:
    \[
\E[d(u,v)] = \sum^{n}_{i=1} k \frac{\binom{n}{k}}{2^n} = \frac{n}{2}
    \]
 

    Let's prove the following result, for any two random vertices $u,v \in V    $, $\epsilon > 0$:

    \[
        \P\left[(1-\epsilon)\frac{n}{2} \leq d(u,v) \leq (1+\epsilon)\frac{n}{2}\right]
        \geq 1 - 2 e^{-\frac{n \epsilon^2 }{6}}  
    \]

\begin{tcolorbox}
   \textbf{Proof:}  

    Let's define the events $E_1$ and $E_2$ as follows:


    \[
        E_1 = \{ (1-\epsilon) \frac{n}{2} \leq d(u,v)\} 
        = \{ (1-\epsilon) \E[d(u,v)] \leq d(u,v)\}
    \]
    And:

    \[
        E_2    = \{ (1+\epsilon) \frac{n}{2} \geq d(u,v)\} 
        = \{(1+\epsilon) \E[d(u,v)] \geq d(u,v)\}    
    \]

We are looking for to bound the probablity: $\P[E_1 \cap E_2]$

\begin{align*}
    \P[E_1 \cap E_2] 
    &= 1 - \P[\overline{E_1} \cup \overline{E_2}] \\
    &\geq 1 - (\P[\overline{E_1}] +  \P[\overline{E_2}]) \\
    &\geq  1 - 2 e^{-\E[d(u,v)]\epsilon^2 / 3} \\
    &\geq  1 - 2 e^{-\frac{n \epsilon^2 }{6}} 
\end{align*}

Here we used the union bound followed by Chernoff 
(Ineq 2. Slide 4 to bound $\P[\overline{E_2}]$, and Ineq 3 for $\P[\overline{E_1}]$), assumig $0 < \epsilon < 1$.
\end{tcolorbox}


Let $V = \{v_1, \dots, v_{2^n}\}$, \textbf{we uniformely random pick n vertices from V }.  Now, let's define the RVs $\{X_i\}$, $\{Y_{i,j}\}$ and $V(v_1, \dots, v_{2^n})$ as follows:

\[
X_i = 
\begin{cases}
    1 \quad \text{if $v_i$ was picked } \\
    0 \quad \text{Otherwise.}
\end{cases}
Y_{i,j} = 
\begin{cases}
    1 \quad \text{if } \quad
    (1-\epsilon)\frac{n}{2} \leq d(v_i, v_j) \leq (1+\epsilon)\frac{n}{2} \\
    0 \quad \text{Otherwise.}
\end{cases}
\]
And finally:
\[
S(v_1, \dots, v_{2^n}) = \sum_{i < j} X_i X_j Y_{i,j}
\]
We need to proof that:

\begin{tcolorbox}    
\textbf{Proof:} 
We have:
\begin{align*}
    \E[S(v_1, \dots, v_{2^n})] 
    &= \sum_{i < j} \E[X_i X_j Y_{i,j}] \\
    &= \sum_{i < j} \E[X_i] \E[X_j] \E[Y_{i,j}] 
    \quad \text{(every rv is independent from the other)}\\
    &= \E[X_1]^2 \sum_{i < j} \E[Y_{i,j}] \\
    &\geq \E[X_1]^2 (1 - 2 e^{-\frac{n \epsilon^2 }{6}})  \sum_{i < j} 1 \\ 
    &= \P[\{\text{Probb. to pick $v_2$}\}]^2 (1 - 2 e^{-\frac{n \epsilon^2 }{6}})  
    \frac{2^{2n} - 2^n}{2} \\ 
    &= \left(\frac{n}{2^n}\right)^2 (1 - 2 e^{-\frac{n \epsilon^2 }{6}})  
    \frac{2^{2n} - 2^n}{2}
    = \frac{n^2}{2}  (1 - 2 e^{-\frac{n \epsilon^2 }{6}})(1 - \frac{1}{2^n} ) \\ 
    &= \frac{n(n-1)}{2} \frac{n}{n-1}  (1 - 2 e^{-\frac{n \epsilon^2 }{6}})(1 - \frac{1}{2^n} ) \\ 
    &= \frac{n(n-1)}{2} 
    (1 + \frac{1}{n-1})(1 - 2 e^{-\frac{n \epsilon^2 }{6}})(1 - \frac{1}{2^n} ) \\ 
\end{align*}

\textbf{fix this:} 

For $n$ big enough, 
$(1 + \frac{1}{n-1})(1 - 2 e^{-\frac{n \epsilon^2 }{6}})(1 - \frac{1}{2^n} )$ is guarenteed to be $\geq 1$

Hence, $\forall \epsilon > 0$, and  with $n$ big enough, $\E[S(v_1, \dots, v_{2^n})] \geq \frac{n(n-1)}{2}  $, hence there exist a configuration of $X, Y$ s.t $v() = \frac{n(n-1)}{2} $, which can only happen if all the n picks had $Y_{i,j} =1$
\end{tcolorbox}
\end{exo}

\end{document}
\maketitle
